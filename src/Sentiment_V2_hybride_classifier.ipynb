{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./StanceDataset/test_ingrid.csv')\n",
    "df_train = pd.read_csv('./StanceDataset/train_ingrid.csv')\n",
    "\n",
    "test_tweet = df_test['Tweet']\n",
    "train_tweet = df_train['Tweet']\n",
    "\n",
    "train_sentiment = df_train['Sentiment']\n",
    "test_sentiment = df_test['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste = []\n",
    "\n",
    "ListeTweet = df_train.Tweet.values.tolist()\n",
    "ListSentiment = df_train.Sentiment.values.tolist()\n",
    "dim = len(ListSentiment)\n",
    "i = 0\n",
    "#print(dim)\n",
    "\n",
    "while i < dim:\n",
    "    tmp = (ListeTweet[i], ListSentiment[i])\n",
    "    liste.append(tmp)\n",
    "    i += 1\n",
    "\n",
    "#Liste de tuples (tweet,sentiment)\n",
    "#print(liste)\n",
    "\n",
    "# liste : Liste(tweet entier, sentiment)\n",
    "\n",
    "\n",
    "tweets = []\n",
    "\n",
    "for (words, sentiment) in liste:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 2]  #1 pour avoir bcp de mot split\n",
    "    tweets.append((words_filtered, sentiment))\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tweets))\n",
    "\n",
    "\n",
    "tokens = get_words_in_tweets(tweets)\n",
    "freq = nltk.FreqDist(tokens) \n",
    "#for key,val in freq.items(): \n",
    "   #print (str(key) + ':' + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_posTweet = []\n",
    "X_train_negTweet = []\n",
    "X_train_otherTweet = []\n",
    "\n",
    "X_test_posTweet = []\n",
    "X_test_negTweet = []\n",
    "X_test_otherTweet = []\n",
    "\n",
    "\n",
    "dim = len(df_train)\n",
    "dim = len(df_test)\n",
    "\n",
    "for i in range(dim):\n",
    "    if train_sentiment[i] == 'pos':\n",
    "        X_train_posTweet.append(train_tweet[i])\n",
    "    elif train_sentiment[i] == 'neg':\n",
    "        X_train_negTweet.append(train_tweet[i])\n",
    "    elif  train_sentiment[i] == 'other':\n",
    "        X_train_otherTweet.append(train_tweet[i])\n",
    "\n",
    "for i in range(dim):        \n",
    "    if test_sentiment[i] == 'pos':\n",
    "        X_test_posTweet.append(test_tweet[i])\n",
    "    elif  test_sentiment[i] == 'neg':\n",
    "        X_test_negTweet.append(test_tweet[i])\n",
    "    elif  test_sentiment[i] == 'other':\n",
    "        X_test_otherTweet.append(test_tweet[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_wordPos = []\n",
    "X_train_wordNeg = []\n",
    "X_train_wordOther = []\n",
    "\n",
    "X_test_wordPos = []\n",
    "X_test_wordNeg = []\n",
    "X_test_wordOther = []\n",
    "\n",
    "\n",
    "for (words) in X_train_posTweet:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 8]  #6 : longueur des mots que l'on prends\n",
    "    X_train_wordPos.append(words_filtered)\n",
    "\n",
    "\n",
    "for (words) in X_train_negTweet:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 8]  #6 : longueur des mots que l'on prends\n",
    "    X_train_wordNeg.append(words_filtered)\n",
    "    \n",
    "\n",
    "for (words) in X_train_otherTweet:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 8]  #6 : longueur des mots que l'on prends\n",
    "    X_train_wordOther.append(words_filtered)\n",
    "    \n",
    "\n",
    "    \n",
    "for (words) in X_test_posTweet:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 8]  #6 : longueur des mots que l'on prends\n",
    "    X_test_wordPos.append(words_filtered)\n",
    "\n",
    "\n",
    "for (words) in X_test_negTweet:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 8]  #6 : longueur des mots que l'on prends\n",
    "    X_test_wordNeg.append(words_filtered)\n",
    "    \n",
    "\n",
    "for (words) in X_test_otherTweet:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 8]  #6 : longueur des mots que l'on prends\n",
    "    X_test_wordOther.append(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.82208588957055\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "    \n",
    "#Featuresets are typically constructed using a \"feature detector\"\n",
    "#(also known as a \"feature extractor\").  A feature detector is a\n",
    "#function that takes a token (and sometimes information about its\n",
    "#context) as its input, and returns a featureset describing that token.\n",
    "#For example, the following feature detector converts a document\n",
    "#(stored as a list of words) to a featureset describing the set of\n",
    "#words included in the document:\n",
    " \n",
    "  #  >>> # Define a feature detector function.\n",
    "   # >>> def document_features(document):\n",
    "   # ...     return dict([('contains-word(%s)' % w, True) for w in document])\n",
    "\n",
    "\n",
    "#Bag of Words Feature Extraction\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    " \n",
    "positive_features_train = [(word_feats(pos), 'pos') for pos in X_train_wordPos]\n",
    "negative_features_train = [(word_feats(neg), 'neg') for neg in X_train_wordNeg]\n",
    "neutral_features_train = [(word_feats(other), 'other') for other in X_train_wordOther]\n",
    "\n",
    "positive_features_test = [(word_feats(pos), 'pos') for pos in X_test_wordPos]\n",
    "negative_features_test = [(word_feats(neg), 'neg') for neg in X_test_wordNeg]\n",
    "neutral_features_test = [(word_feats(other), 'other') for other in X_test_wordOther]\n",
    " \n",
    "train_set = negative_features_train + positive_features_train + neutral_features_train\n",
    "test_set = negative_features_test + positive_features_test + neutral_features_test\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    "\n",
    "#print(nltk.classify.accuracy(classifier,test_set))\n",
    "\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "print(accuracy * 100)\n",
    "#classifier.show_most_informative_features()\n",
    "# ACCURACY : taux de bonne prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A learning algorithm is very useful for a classifier, here we will show you how to use the Naive Bayes and Maximum Entropy Model to train a NaiveBayes and Maxent Classifier, where Naive Bayes is the Generative Model and Maxent is Discriminative Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.351\n",
      "             2          -0.56450        0.928\n",
      "             3          -0.42090        0.946\n",
      "             4          -0.34260        0.955\n",
      "             5          -0.29257        0.960\n",
      "             6          -0.25764        0.964\n",
      "             7          -0.23176        0.964\n",
      "             8          -0.21175        0.966\n",
      "             9          -0.19579        0.966\n",
      "            10          -0.18273        0.966\n",
      "            11          -0.17183        0.967\n",
      "            12          -0.16258        0.967\n",
      "            13          -0.15462        0.968\n",
      "            14          -0.14770        0.968\n",
      "            15          -0.14161        0.968\n",
      "            16          -0.13622        0.968\n",
      "            17          -0.13140        0.968\n",
      "            18          -0.12707        0.968\n",
      "            19          -0.12315        0.968\n",
      "            20          -0.11958        0.968\n",
      "            21          -0.11633        0.968\n",
      "            22          -0.11334        0.968\n",
      "            23          -0.11058        0.968\n",
      "            24          -0.10804        0.968\n",
      "            25          -0.10567        0.968\n",
      "            26          -0.10347        0.968\n",
      "            27          -0.10142        0.968\n",
      "            28          -0.09950        0.968\n",
      "            29          -0.09771        0.968\n",
      "            30          -0.09601        0.968\n",
      "            31          -0.09442        0.968\n",
      "            32          -0.09292        0.968\n",
      "            33          -0.09150        0.968\n",
      "            34          -0.09015        0.968\n",
      "            35          -0.08888        0.968\n",
      "            36          -0.08767        0.968\n",
      "            37          -0.08651        0.968\n",
      "            38          -0.08542        0.968\n",
      "            39          -0.08437        0.968\n",
      "            40          -0.08337        0.968\n",
      "            41          -0.08242        0.968\n",
      "            42          -0.08150        0.968\n",
      "            43          -0.08063        0.968\n",
      "            44          -0.07979        0.968\n",
      "            45          -0.07898        0.968\n",
      "            46          -0.07821        0.968\n",
      "            47          -0.07747        0.968\n",
      "            48          -0.07675        0.968\n",
      "            49          -0.07606        0.968\n",
      "            50          -0.07540        0.968\n",
      "            51          -0.07476        0.968\n",
      "            52          -0.07414        0.969\n",
      "            53          -0.07354        0.969\n",
      "            54          -0.07297        0.969\n",
      "            55          -0.07241        0.969\n",
      "            56          -0.07187        0.969\n",
      "            57          -0.07135        0.969\n",
      "            58          -0.07084        0.969\n",
      "            59          -0.07035        0.969\n",
      "            60          -0.06987        0.969\n",
      "            61          -0.06941        0.969\n",
      "            62          -0.06897        0.970\n",
      "            63          -0.06853        0.970\n",
      "            64          -0.06811        0.970\n",
      "            65          -0.06770        0.970\n",
      "            66          -0.06730        0.970\n",
      "            67          -0.06691        0.970\n",
      "            68          -0.06653        0.970\n",
      "            69          -0.06617        0.970\n",
      "            70          -0.06581        0.970\n",
      "            71          -0.06546        0.970\n",
      "            72          -0.06512        0.970\n",
      "            73          -0.06479        0.970\n",
      "            74          -0.06446        0.970\n",
      "            75          -0.06415        0.970\n",
      "            76          -0.06384        0.970\n",
      "            77          -0.06354        0.970\n",
      "            78          -0.06325        0.970\n",
      "            79          -0.06296        0.970\n",
      "            80          -0.06268        0.970\n",
      "            81          -0.06241        0.970\n",
      "            82          -0.06214        0.970\n",
      "            83          -0.06188        0.970\n",
      "            84          -0.06162        0.970\n",
      "            85          -0.06137        0.970\n",
      "            86          -0.06112        0.970\n",
      "            87          -0.06088        0.970\n",
      "            88          -0.06065        0.970\n",
      "            89          -0.06042        0.970\n",
      "            90          -0.06019        0.970\n",
      "            91          -0.05997        0.970\n",
      "            92          -0.05976        0.970\n",
      "            93          -0.05954        0.970\n",
      "            94          -0.05934        0.970\n",
      "            95          -0.05913        0.970\n",
      "            96          -0.05893        0.970\n",
      "            97          -0.05874        0.970\n",
      "            98          -0.05854        0.970\n",
      "            99          -0.05835        0.970\n",
      "         Final          -0.05817        0.970\n",
      "0.4994887525562372\n"
     ]
    }
   ],
   "source": [
    "from nltk import MaxentClassifier\n",
    " \n",
    "entropy_classifier = MaxentClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(entropy_classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier pourcentage de bonne prédiction : 63.75255623721882\n",
      "BernoulliNB_classifier pourcentage de bonne prédiction : 65.03067484662577\n",
      "LogisticRegression_classifier pourcentage de bonne prédiction : 64.62167689161554\n",
      "SGDClassifier_classifier pourcentage de bonne prédiction : 61.40081799591002\n",
      "SVC_classifier pourcentage de bonne prédiction : 65.03067484662577\n",
      "LinearSVC_classifier pourcentage de bonne prédiction : 60.88957055214724\n"
     ]
    }
   ],
   "source": [
    "#print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, test_set))*100)\n",
    "#classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(train_set)\n",
    "print(\"MNB_classifier pourcentage de bonne prédiction :\", (nltk.classify.accuracy(MNB_classifier, test_set))*100)\n",
    "#The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(train_set)\n",
    "print(\"BernoulliNB_classifier pourcentage de bonne prédiction :\", (nltk.classify.accuracy(BernoulliNB_classifier, test_set))*100)\n",
    "#Naive Bayes classifier for multivariate Bernoulli models. Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(train_set)\n",
    "print(\"LogisticRegression_classifier pourcentage de bonne prédiction :\", (nltk.classify.accuracy(LogisticRegression_classifier, test_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(train_set)\n",
    "print(\"SGDClassifier_classifier pourcentage de bonne prédiction :\", (nltk.classify.accuracy(SGDClassifier_classifier, test_set))*100)\n",
    "\n",
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(train_set)\n",
    "print(\"SVC_classifier pourcentage de bonne prédiction :\", (nltk.classify.accuracy(SVC_classifier, test_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(train_set)\n",
    "print(\"LinearSVC_classifier pourcentage de bonne prédiction :\", (nltk.classify.accuracy(LinearSVC_classifier, test_set))*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHODE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_tweet\n",
    "train_y = train_sentiment\n",
    " \n",
    "test_X = test_tweet\n",
    "test_y = test_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \n",
    "    #Convert to simple Wordnet tags\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_y)):        \n",
    "    if train_y[i] == 'pos':\n",
    "        train_y[i] = 1\n",
    "    elif  train_y[i] == 'neg':\n",
    "        train_y[i] = 0\n",
    "    elif  train_y[i] == 'other':\n",
    "        train_y[i] = 2\n",
    "        \n",
    "for i in range(len(test_y)):        \n",
    "    if test_y[i] == 'pos':\n",
    "        test_y[i] = 1\n",
    "    elif  test_y[i] == 'neg':\n",
    "        test_y[i] = 0\n",
    "    elif  test_y[i] == 'other':\n",
    "        test_y[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swn_polarity(text):\n",
    "\n",
    "    #Return a sentiment polarity: 0 = negative, 1 = positive\n",
    "\n",
    " \n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    " \n",
    "\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    #print (\"raw_sentences\")\n",
    "    #print (raw_sentences)\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        #print (\"word_tokenize(raw_sentence)\")\n",
    "        #print (word_tokenize(raw_sentence))\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "        #print (\"tagged_sentence\")\n",
    "        #print (tagged_sentence)\n",
    "        \n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            #print (\"wn_tag\")\n",
    "            #print (wn_tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    " \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            #print (\"lemma\")\n",
    "            #print (lemma)\n",
    "            \n",
    "            if not lemma:\n",
    "                continue\n",
    " \n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            #print (\"synsets\")\n",
    "            #print (synsets)\n",
    "    \n",
    "            if not synsets:\n",
    "                continue\n",
    " \n",
    "            # Take the first sense, the most common\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            #print (\"swn_synset\")\n",
    "            #print (swn_synset)\n",
    "\n",
    "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            tokens_count += 1\n",
    "            #print (\"sentiment\")\n",
    "            #print (sentiment)\n",
    " \n",
    "\n",
    "    # judgment call ? Default to positive or negative\n",
    "    #if not tokens_count:\n",
    "        #return 0\n",
    " \n",
    "    # sum greater than 0 => positive sentiment\n",
    "    #if sentiment >= 0:\n",
    "        #return 1\n",
    " \n",
    "    # negative sentiment\n",
    "    #return 0\n",
    "    if sentiment < 0:\n",
    "        return 0\n",
    "    elif sentiment > 0:\n",
    "        return 1\n",
    "    elif sentiment == 0:\n",
    "        return 2\n",
    " \n",
    "#print (swn_polarity(train_X[0]), train_y[0])\n",
    "\n",
    "dim = len(train_X)\n",
    "Nberreur = 0\n",
    "\n",
    "for j in range(dim):\n",
    "    if swn_polarity(train_X[j]) != train_y[j]:\n",
    "        Nberreur=Nberreur+1\n",
    "    j=j+1 \n",
    "\n",
    "taux_erreur = 1-(dim-Nberreur)/dim\n",
    "\n",
    "print(taux_erreur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHODE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = zip(df_train[\"Tweet\"], df_train[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
