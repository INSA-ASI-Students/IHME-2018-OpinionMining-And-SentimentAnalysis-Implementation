{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "import re, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer        \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./StanceDataset/test_ingrid.csv')\n",
    "df_train = pd.read_csv('./StanceDataset/train_ingrid.csv')\n",
    "\n",
    "test_tweet = df_test['Tweet']\n",
    "train_tweet = df_train['Tweet']\n",
    "\n",
    "train_sentiment = df_train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([len(s.split(\" \")) for s in train_tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#semst', 2814), ('that', 333), ('have', 240), ('with', 224), ('your', 209), ('this', 184), ('will', 176), (\"don't\", 164), ('they', 160), ('about', 157), ('what', 152), ('just', 149), (\"it's\", 134), ('people', 129), ('when', 126), ('like', 124), ('women', 119), ('their', 118), ('@hillaryclinton', 111), ('from', 109)]\n",
      "[('#semst', 2814), (\"don't\", 164), (\"it's\", 134), ('people', 129), ('like', 124), ('women', 119), ('@hillaryclinton', 111), ('hillary', 99), ('want', 96), ('know', 81), ('need', 80), ('right', 77), (\"can't\", 76), ('would', 75), ('feminists', 74), ('make', 72), ('abortion', 71), ('life', 68), ('think', 66), ('love', 64)]\n"
     ]
    }
   ],
   "source": [
    "liste = []\n",
    "\n",
    "ListeTweet = df_train.Tweet.values.tolist()\n",
    "ListSentiment = df_train.Sentiment.values.tolist()\n",
    "dim = len(ListSentiment)\n",
    "i = 0\n",
    "#print(dim)\n",
    "\n",
    "while i < dim:\n",
    "    tmp = (ListeTweet[i], ListSentiment[i])\n",
    "    liste.append(tmp)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# liste : Liste(tweet entier, sentiment)\n",
    "# tweets : Liste(Liste des mots du tweet, sentiment)\n",
    "\n",
    "tweets = []\n",
    "\n",
    "for (words, sentiment) in liste:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 4]  #1 pour avoir bcp de mot split\n",
    "    tweets.append((words_filtered, sentiment))\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tweets))\n",
    "#word_features : dictionnaire de mots à partir des tweets\n",
    "\n",
    "tokens = get_words_in_tweets(tweets)\n",
    "\n",
    "count1 = Counter(tokens)\n",
    "print (Counter(count1).most_common(20))\n",
    "## interessant une fois les mots neutre enlevé\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "count2 = Counter(filtered)\n",
    "print(Counter(count2).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered, stemmer)\n",
    "count3 = Counter(stemmed)\n",
    "print(Counter(count3).most_common(20))\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokens,\n",
    "    lowercase = True,\n",
    "    stop_words = 'english',\n",
    "    max_features = 85\n",
    ")\n",
    "\n",
    "## autre methode que le vectorisation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokens, stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tokens\n",
    "#nltk.pos_tag(w)\n",
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(w)\n",
    "#freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how the Naive Bayes classifier expects the input\n",
    "def create_word_features(words):\n",
    "    useful_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    my_dict = dict([(word, True) for word in useful_words])\n",
    "    return my_dict\n",
    "#For each word, we create a dictionary with all the words and True. Why a dictionary? So that words are not repeated. If a word already exists, it won’t be added to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## METHODE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words=['a', 'able','about','across','after','all','almost','also','am','among','an','and','any','are','as','at','be','because','been','by','did','else','ever','every','for','from','get','got','had','has','have','he','her','hers','him','his','how','however','i','if','in','into','is','it','its','just','least','let','may','me','might','my','of','off','on','or','other','our','own','rather','said','say','says','she','should','since','so','than','that','the','their','them','then','there','these','they','this','tis','to','was','us','was','we','were','what','when','where','while','who','whom','why','will','would','yet','you','your','They','Look','Good','A', 'Able','About','Across','After','All','Almost','Also','Am','Among','An','And','Any','Are','As','At','Be','Because','Been','By','Did','Else','Ever','Every','For','From','Get','Got','Had','Has','Have','He','Her','Hers','Him','His','How','However','I','If','In','Into','Is','It','Its','Just','Least','Let','May','Me','Might','My','Of','Off','On','Or','Other','Our','Own','Rather','Said','Say','Says','She','Should','Since','So','Than','That','The','Their','Them','Then','There','These','They','This','Tis','To','Was','Us','Was','We','Were','What','When','Where','While','Who','Whom','Why','Will','Would','Yet','You','Your','!','@','#','\"','$','(','.',')']                                                                                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureVector(tweet):\n",
    "    featureVector = []\n",
    "    \n",
    "    #extraction des mots\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if the word stats with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stop word\n",
    "        if(w in stop_words or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "tweets = []\n",
    "featureList = []\n",
    "for i in range(len(df_train)):\n",
    "    sentiment = df_train['Sentiment'][i]\n",
    "    tweet = df_train['Tweet'][i]\n",
    "    ## faire un pré-traitement\n",
    "    processedTweet = tweet\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    featureList.extend(featureVector)\n",
    "    tweets.append((featureVector, sentiment))\n",
    "        \n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "### Remove featureList duplicates\n",
    "featureList = list(set(featureList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_set = nltk.classify.util.apply_features(extract_features, tweets)\n",
    "\n",
    "#NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "#df_train['Sentiment'] = df_train['Tweets'].apply(lambda tweet: NBClassifier.classify(extract_features(getFeatureVector(tweet))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# METHODE 2\n",
    "## Sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tweet_pretraitement = [item.lower() for item in train_tweet]\n",
    "df = pd.DataFrame(train_tweet_pretraitement)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "\n",
    "liste = []\n",
    "\n",
    "ListeTweet = df_train.Tweet.values.tolist()\n",
    "ListSentiment = df_train.Sentiment.values.tolist()\n",
    "dim = len(ListSentiment)\n",
    "i = 0\n",
    "#print(dim)\n",
    "\n",
    "while i < dim:\n",
    "    tmp = (ListeTweet[i], ListSentiment[i])\n",
    "    liste.append(tmp)\n",
    "    i += 1\n",
    "\n",
    "#Liste de tuples (tweet,sentiment)\n",
    "#print(liste)\n",
    "\n",
    "# liste : Liste(tweet entier, sentiment)\n",
    "\n",
    "\n",
    "tweets = []\n",
    "\n",
    "for (words, sentiment) in liste:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 6]  #1 pour avoir bcp de mot split\n",
    "    tweets.append((words_filtered, sentiment))\n",
    "\n",
    "#len(tweets)\n",
    "#print(tweets)\n",
    "\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tweets))\n",
    "# tweets : Liste(mot du tweet, sentiment)\n",
    "#word_features : Dictionnaire des mots de plus de 6 lettres dans tous les tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### AUTRE METHODE ###\n",
    "\n",
    "import nltk\n",
    "#nltk.download('all')\n",
    "\n",
    "def tokenization(liste):\n",
    "    token = []\n",
    "    for (words, sentiment) in liste:\n",
    "        tmp = nltk.word_tokenize(words)\n",
    "        token.extend(tmp)\n",
    "    return token\n",
    "\n",
    "#token = nltk.word_tokenize(liste)\n",
    "#tagged = nltk.pos_tag(token)\n",
    "\n",
    "all_words = tokenization(liste)\n",
    "tagged = nltk.pos_tag(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#print(\"\\n\\n\")\n",
    "df=pd.read_csv(\"./StanceDataset/train_ingrid.csv\")\n",
    "\n",
    "#print(\"Dataset.................\")\n",
    "#print (df)\n",
    "#print(\"\\n\\n\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(\"lower..............\\n\\n\")\n",
    "texts = df['Tweet'].map(lambda Review_Text: Review_Text.lower())#...............converting to   lower case\n",
    "#print(texts)\n",
    "df['texts']=texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(\"Tokenisation..............\\n\\n\")\n",
    "token = df.apply(lambda row: nltk.word_tokenize(row['texts']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tokenized_words']=token\n",
    "#print(df['tokenized_words'])\n",
    "#print(\"stopwords removal///////////////////////\")\n",
    "words=df['tokenized_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "usertext = token\n",
    "stop_words=['a', 'able','about','across','after','all','almost','also','am','among','an','and','any','are','as','at','be','because','been','by','did','else','ever','every','for','from','get','got','had','has','have','he','her','hers','him','his','how','however','i','if','in','into','is','it','its','just','least','let','may','me','might','my','of','off','on','or','other','our','own','rather','said','say','says','she','should','since','so','than','that','the','their','them','then','there','these','they','this','tis','to','was','us','was','we','were','what','when','where','while','who','whom','why','will','would','yet','you','your','They','Look','Good','A', 'Able','About','Across','After','All','Almost','Also','Am','Among','An','And','Any','Are','As','At','Be','Because','Been','By','Did','Else','Ever','Every','For','From','Get','Got','Had','Has','Have','He','Her','Hers','Him','His','How','However','I','If','In','Into','Is','It','Its','Just','Least','Let','May','Me','Might','My','Of','Off','On','Or','Other','Our','Own','Rather','Said','Say','Says','She','Should','Since','So','Than','That','The','Their','Them','Then','There','These','They','This','Tis','To','Was','Us','Was','We','Were','What','When','Where','While','Who','Whom','Why','Will','Would','Yet','You','Your','!','@','#','\"','$','(','.',')']                                                                                               \n",
    "clean = usertext.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "df['clean']=clean\n",
    "#print(df['clean'])\n",
    "#print(\"\\n\")\n",
    "#print(\"Pos_Tagging..............\\n\\n\")\n",
    "df['tagged_texts'] = df.apply(lambda df:nltk.pos_tag(df['clean']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(\"\\n\\n\")\n",
    "#print(\"Array..............\\n\\n\")\n",
    "tagged=np.array(df['tagged_texts'])\n",
    "#print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "dim = len(tagged)\n",
    "result_sentiment = []\n",
    "\n",
    "\n",
    "while i < dim:\n",
    "    \n",
    "    for word, tag in tagged[i]:\n",
    "        try:\n",
    "            ss_set = None\n",
    "            #print(list(swn.senti_synsets(word))[0])\n",
    "            if 'NN' in tag and swn.senti_synsets(word):\n",
    "                ss_set = list(swn.senti_synsets(word))[0]\n",
    "            elif 'VB' in tag and swn.senti_synsets(word):\n",
    "                ss_set = list(swn.senti_synsets(word))[0]\n",
    "            elif 'JJ' in tag and swn.senti_synsets(word):\n",
    "                 ss_set = list(swn.senti_synsets(word))[0]\n",
    "            elif 'RB' in tag and swn.senti_synsets(word):\n",
    "                 ss_set = list(swn.senti_synsets(word))[0]\n",
    "\n",
    "            if ss_set:\n",
    "                pos=pos+ss_set.pos_score()\n",
    "                neg=neg+ss_set.neg_score()\n",
    "                obj=obj+ss_set.obj_score()\n",
    "                count+=1\n",
    "\n",
    "            #print(i)             \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    final_score=pos-neg\n",
    "    #print(final_score)\n",
    "    #print(\"Score sentiment du tweet n°\" + i + \" : \" + final_score)\n",
    "    print(\"Score sentiment du tweet n° %d : %s  \" % (i, final_score))\n",
    "    df['final_score']=final_score\n",
    "    df.to_csv('score_sentiment_save.csv')\n",
    "    \n",
    "    if final_score < 0 :\n",
    "        result_sentiment.append('neg')\n",
    "    elif final_score > 0 :\n",
    "        result_sentiment.append('pos')\n",
    "    elif final_score == 0 :\n",
    "        result_sentiment.append('other')\n",
    "    \n",
    "    print(result_sentiment[i])\n",
    "    \n",
    "    #norm_finalscore= round((final_score) / count, 2)\n",
    "    #print(norm_finalscore)\n",
    "    #final_sentiment = 'positive' if norm_finalscore >= 0 else 'negative'\n",
    "    #print(final_sentiment)    \n",
    "    i+=1\n",
    "    pos=neg=obj=count=0\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_true=np.array(df_train['Sentiment'])\n",
    "erreur = 0\n",
    "dim = len(label_true)\n",
    "\n",
    "for j in range(dim):\n",
    "    if label_true[j] != result_sentiment[j]:\n",
    "        erreur=erreur+1\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dim)\n",
    "print(erreur)\n",
    "\n",
    "taux_erreur = (dim-erreur)/dim\n",
    "\n",
    "print(taux_erreur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Erreur %s  \" % (taux_erreur))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
